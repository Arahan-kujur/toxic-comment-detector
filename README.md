# ðŸ§  Toxic Comment Detector (NLP Project)

This project is a **Natural Language Processing (NLP)** tool that classifies toxic comments. It features **two different models**:

1. âœ… **TF-IDF + Logistic Regression** â€“ A lightweight traditional machine learning model.
2. ðŸ¤– **DistilBERT (Transformer)** â€“ A deep learning model fine-tuned on the Jigsaw Toxic Comment dataset.

Users can train and compare both models and interact with the BERT version through a web interface.

---

## ðŸ“‚ Project Structure

- `main.py` â€“ Implements the classic ML pipeline (TF-IDF + Logistic Regression)
- `bert_toxic.py` â€“ Trains a DistilBERT model on toxic comment data
- `bert_gradio.py` â€“ Launches a Gradio-based UI for the DistilBERT model
- `train.csv` â€“ Dataset used for training (download separately)
- `toxic-bert/` â€“ Directory containing saved BERT model weights and tokenizer
- `requirements.txt` â€“ List of required Python packages
- `.gitignore` â€“ File/folder ignore rules for version control

---

## ðŸ“Š Dataset

This project uses the **Jigsaw Toxic Comment Classification Challenge** dataset.

Due to GitHub file size limits, you must manually download the dataset:

ðŸ”— [Download train.csv from Kaggle](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)

Place the `train.csv` file in the root directory before running the code.

---

## ðŸš€ How to Run

### 1. Install Dependencies

```bash
pip install -r requirements.txt
